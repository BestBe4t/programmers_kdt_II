{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], Y[:60000], Y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = X_train[:50000], X_train[50000:], y_train[:50000], y_train[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.hstack((np.ones((np.size(X_valid, 0),1)),X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 785)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1))) # reshape(, -1) -> (10, 1) -> (10)\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, L2_Regular):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    # 기존 cost + L2\n",
    "    cost = - (1/N) * np.ones((1,N)) @ ((np.multiply(np.log(softmax(X, W) + epsilon), T))) @ np.ones((K,1)) + L2_Regular\n",
    "    #cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef batch_gd(X, T, W, learning_rate, iterations, batch_size):\\n    N = len(T)\\n    cost_history = np.zeros((iterations,1))\\n    shuffled_indices = np.random.permutation(N)\\n    X_shuffled = X[shuffled_indices]\\n    T_shuffled = T[shuffled_indices]\\n    \\n    for i in range(iterations):\\n        j = i % N\\n        X_batch = X_shuffled[j:j+batch_size]\\n        T_batch = T_shuffled[j:j+batch_size]\\n        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\\n        if X_batch.shape[0] < batch_size:\\n            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\\n            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\\n        \\n        # lambda 적용..?\\n        valid = predict(X_valid, W)\\n        \\n        #lamda = float(sum(valid == np.argmax(y_valid, axis=1)))/ float(len(y_valid))\\n        #lamda = (1 - float(sum(valid == np.argmax(y_valid, axis=1)))/ float(len(y_valid))) * 0.001\\n        lamda = 0\\n        # L2 구하기: W 제곱의 합 * lambda/2n\\n        \\n        L2_Regular = (lamda/(2 * len(X))) * np.ones((1, len(W))) @ (W ** 2) @ np.ones((np.size(W, 1), 1))\\n        L2_norm_d = W * 2 * lamda # L2 norm의 미분값\\n        \\n        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch) + L2_norm_d)\\n        # W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch)) \\n        cost_history[i] = compute_cost(X_batch, T_batch, W, L2_Regular)\\n        if i % 1000 == 0:\\n            #print(cost_history[i][0])\\n            print(\"lamda\", lamda)\\n            print(\"L2_norm_d\", np.ones((1, len(W))) @ W ** 2 @ np.ones((np.size(W, 1), 1)))\\n            print(\"-------------\")\\n\\n    return (cost_history, W)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        \n",
    "        # lambda 적용..?\n",
    "        valid = predict(X_valid, W)\n",
    "        \n",
    "        #lamda = float(sum(valid == np.argmax(y_valid, axis=1)))/ float(len(y_valid))\n",
    "        #lamda = (1 - float(sum(valid == np.argmax(y_valid, axis=1)))/ float(len(y_valid))) * 0.001\n",
    "        lamda = 0\n",
    "        # L2 구하기: W 제곱의 합 * lambda/2n\n",
    "        \n",
    "        L2_Regular = (lamda/(2 * len(X))) * np.ones((1, len(W))) @ (W ** 2) @ np.ones((np.size(W, 1), 1))\n",
    "        L2_norm_d = W * 2 * lamda # L2 norm의 미분값\n",
    "        \n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch) + L2_norm_d)\n",
    "        # W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch)) \n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W, L2_Regular)\n",
    "        if i % 1000 == 0:\n",
    "            #print(cost_history[i][0])\n",
    "            print(\"lamda\", lamda)\n",
    "            print(\"L2_norm_d\", np.ones((1, len(W))) @ W ** 2 @ np.ones((np.size(W, 1), 1)))\n",
    "            print(\"-------------\")\n",
    "\n",
    "    return (cost_history, W)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "    max_acc = 0\n",
    "    max_lamda = 0\n",
    "    x_acc = 0\n",
    "    max_W = np.zeros((np.size(X, 1), np.size(T, 1)))\n",
    "    lamda = 0\n",
    "    \n",
    "    for k in range(101):\n",
    "        W = np.zeros((np.size(X, 1), np.size(T, 1)))\n",
    "        cnt1 = 0\n",
    "        cnt2 = 0\n",
    "        print(\"lambda\", lamda)\n",
    "        for i in range(iterations):\n",
    "            j = i % N\n",
    "            X_batch = X_shuffled[j:j+batch_size]\n",
    "            T_batch = T_shuffled[j:j+batch_size]\n",
    "            # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "            if X_batch.shape[0] < batch_size:\n",
    "                X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "                T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "            \n",
    "            L2_Regular = lamda * math.sqrt(np.ones((1, len(W))) @ (W ** 2) @ np.ones((np.size(W, 1), 1)))\n",
    "\n",
    "            W = W * (1 - 2 * learning_rate/batch_size * lamda) - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch)) \n",
    "            cost_history[i] = compute_cost(X_batch, T_batch, W, L2_Regular)\n",
    "            #if i % 1000 == 0:\n",
    "                #print(\"{} %...\".format(i/200))\n",
    "\n",
    "        y_pred = predict(X_valid, W)\n",
    "        score = float(sum(y_pred == np.argmax(y_valid, axis=1)))/ float(len(y_valid))\n",
    "        print(\"Sum W^2\", np.ones((1, len(W))) @ W ** 2 @ np.ones((np.size(W, 1), 1)))\n",
    "        print(\"acc\", score)\n",
    "        if max_acc < score:\n",
    "            max_W = W\n",
    "            max_acc = score\n",
    "            if lamda != 0:\n",
    "                lamda -= (1 - score) * 0.05\n",
    "            print(\"max change!!\")\n",
    "        else:\n",
    "            if x_acc > score:\n",
    "                lamda -= (1 - score) * 0.02\n",
    "            else:\n",
    "                lamda += (1 - score) * 0.05\n",
    "        print(\"---------------------------------------\")\n",
    "        if max_acc - score > 0.04:\n",
    "            print(\"early stopping...\")\n",
    "            break\n",
    "        x_acc = score\n",
    "            \n",
    "    print('max lambda: ',max_lamda)\n",
    "    print('max acc: ',max_acc)\n",
    "    return (cost_history, max_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 2.3024850979937126 \n",
      "\n",
      "lambda 0\n",
      "Sum W^2 [[80.24403349]]\n",
      "acc 0.937\n",
      "max change!!\n",
      "---------------------------------------\n",
      "lambda 0\n",
      "Sum W^2 [[80.24403349]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.0031499999999999974\n",
      "Sum W^2 [[78.99902183]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.006299999999999995\n",
      "Sum W^2 [[77.78798384]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.009449999999999993\n",
      "Sum W^2 [[76.60979684]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.01259999999999999\n",
      "Sum W^2 [[75.46337792]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.015749999999999986\n",
      "Sum W^2 [[74.34768248]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.014469999999999986\n",
      "Sum W^2 [[74.79740411]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.017669999999999984\n",
      "Sum W^2 [[73.68227866]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.020869999999999982\n",
      "Sum W^2 [[72.59720441]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.02406999999999998\n",
      "Sum W^2 [[71.54118483]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.02726999999999998\n",
      "Sum W^2 [[70.51325925]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.030469999999999976\n",
      "Sum W^2 [[69.51250154]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.033619999999999976\n",
      "Sum W^2 [[68.55304749]]\n",
      "acc 0.938\n",
      "max change!!\n",
      "---------------------------------------\n",
      "lambda 0.030519999999999974\n",
      "Sum W^2 [[69.49707547]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.029259999999999974\n",
      "Sum W^2 [[69.88777631]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.032409999999999974\n",
      "Sum W^2 [[68.91864105]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.03550999999999998\n",
      "Sum W^2 [[67.98925311]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.038609999999999985\n",
      "Sum W^2 [[67.0832644]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.04170999999999999\n",
      "Sum W^2 [[66.19993574]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.044809999999999996\n",
      "Sum W^2 [[65.33855363]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.04791\n",
      "Sum W^2 [[64.49842939]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.05101000000000001\n",
      "Sum W^2 [[63.67889825]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.05411000000000001\n",
      "Sum W^2 [[62.87931851]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.05721000000000002\n",
      "Sum W^2 [[62.0990707]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.06031000000000002\n",
      "Sum W^2 [[61.33755681]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.059050000000000026\n",
      "Sum W^2 [[61.64485304]]\n",
      "acc 0.938\n",
      "---------------------------------------\n",
      "lambda 0.06215000000000003\n",
      "Sum W^2 [[60.89418431]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.060890000000000034\n",
      "Sum W^2 [[61.19711337]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.06404000000000003\n",
      "Sum W^2 [[60.44529962]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.06719000000000003\n",
      "Sum W^2 [[59.71154294]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.07034000000000003\n",
      "Sum W^2 [[58.99528286]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.07349000000000003\n",
      "Sum W^2 [[58.29597857]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.07664000000000003\n",
      "Sum W^2 [[57.61310812]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.07979000000000003\n",
      "Sum W^2 [[56.94616777]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.08294000000000003\n",
      "Sum W^2 [[56.29467132]]\n",
      "acc 0.937\n",
      "---------------------------------------\n",
      "lambda 0.08609000000000003\n",
      "Sum W^2 [[55.65814948]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.08481000000000002\n",
      "Sum W^2 [[55.91502285]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.08801000000000002\n",
      "Sum W^2 [[55.27732567]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.09121000000000001\n",
      "Sum W^2 [[54.6543378]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.09441000000000001\n",
      "Sum W^2 [[54.04561057]]\n",
      "acc 0.936\n",
      "---------------------------------------\n",
      "lambda 0.09761\n",
      "Sum W^2 [[53.45071107]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.09629\n",
      "Sum W^2 [[53.6944587]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.09959\n",
      "Sum W^2 [[53.08935868]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.10284\n",
      "Sum W^2 [[52.50710665]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.10609\n",
      "Sum W^2 [[51.9380147]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.10477\n",
      "Sum W^2 [[52.16759238]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.10802\n",
      "Sum W^2 [[51.60612595]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.1067\n",
      "Sum W^2 [[51.83263471]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.11\n",
      "Sum W^2 [[51.27023053]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.1133\n",
      "Sum W^2 [[50.72050828]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.1166\n",
      "Sum W^2 [[50.18308241]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.11989999999999999\n",
      "Sum W^2 [[49.65758105]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.12319999999999999\n",
      "Sum W^2 [[49.14364549]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.12644999999999998\n",
      "Sum W^2 [[48.64846453]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.12969999999999998\n",
      "Sum W^2 [[48.16384686]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.13294999999999998\n",
      "Sum W^2 [[47.68948447]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.13163\n",
      "Sum W^2 [[47.88093112]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.13493\n",
      "Sum W^2 [[47.40538818]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.13823\n",
      "Sum W^2 [[46.93993028]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.14153\n",
      "Sum W^2 [[46.48426316]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.14483\n",
      "Sum W^2 [[46.03810274]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.14812999999999998\n",
      "Sum W^2 [[45.60117475]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.15142999999999998\n",
      "Sum W^2 [[45.17321437]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.15472999999999998\n",
      "Sum W^2 [[44.75396582]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.15802999999999998\n",
      "Sum W^2 [[44.34318209]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.16132999999999997\n",
      "Sum W^2 [[43.94062455]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.16462999999999997\n",
      "Sum W^2 [[43.54606265]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.16792999999999997\n",
      "Sum W^2 [[43.15927364]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.17122999999999997\n",
      "Sum W^2 [[42.78004224]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.17447999999999997\n",
      "Sum W^2 [[42.41374111]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.17772999999999997\n",
      "Sum W^2 [[42.05437631]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.17640999999999998\n",
      "Sum W^2 [[42.19950926]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.17970999999999998\n",
      "Sum W^2 [[41.83876087]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.18300999999999998\n",
      "Sum W^2 [[41.48485854]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.18630999999999998\n",
      "Sum W^2 [[41.13761781]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.18960999999999997\n",
      "Sum W^2 [[40.79686026]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.19290999999999997\n",
      "Sum W^2 [[40.4624133]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.19620999999999997\n",
      "Sum W^2 [[40.13410999]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.19950999999999997\n",
      "Sum W^2 [[39.81178877]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.20280999999999996\n",
      "Sum W^2 [[39.49529329]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.20605999999999997\n",
      "Sum W^2 [[39.18914005]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.20930999999999997\n",
      "Sum W^2 [[38.88835063]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.20798999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum W^2 [[39.00987933]]\n",
      "acc 0.935\n",
      "---------------------------------------\n",
      "lambda 0.21123999999999998\n",
      "Sum W^2 [[38.71221076]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.20992\n",
      "Sum W^2 [[38.83248153]]\n",
      "acc 0.934\n",
      "---------------------------------------\n",
      "lambda 0.21322\n",
      "Sum W^2 [[38.53339462]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.21187999999999999\n",
      "Sum W^2 [[38.65420398]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.21522999999999998\n",
      "Sum W^2 [[38.35379398]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.21857999999999997\n",
      "Sum W^2 [[38.058688]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.22192999999999996\n",
      "Sum W^2 [[37.76875031]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.22527999999999995\n",
      "Sum W^2 [[37.48384946]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.22862999999999994\n",
      "Sum W^2 [[37.20385815]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.23197999999999994\n",
      "Sum W^2 [[36.9286531]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.23532999999999993\n",
      "Sum W^2 [[36.65811486]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.23867999999999992\n",
      "Sum W^2 [[36.39212773]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.2420299999999999\n",
      "Sum W^2 [[36.13057957]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.2453799999999999\n",
      "Sum W^2 [[35.8733617]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.2487299999999999\n",
      "Sum W^2 [[35.62036877]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.2520799999999999\n",
      "Sum W^2 [[35.37149865]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "lambda 0.25542999999999993\n",
      "Sum W^2 [[35.12665231]]\n",
      "acc 0.933\n",
      "---------------------------------------\n",
      "max lambda:  0\n",
      "max acc:  0.938\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 20000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W, 0)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "(cost_history, W_optimal) = batch_gd(X, T, learning_rate, iterations, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9039\n"
     ]
    }
   ],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "T_ = y_test\n",
    "y_pred = predict(X_, W_optimal)\n",
    "score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
