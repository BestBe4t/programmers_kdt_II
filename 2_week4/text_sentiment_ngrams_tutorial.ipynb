{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"text_sentiment_ngrams_tutorial.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"mX_PfZ253m8s","executionInfo":{"status":"ok","timestamp":1612439715456,"user_tz":-540,"elapsed":700,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}}},"source":["%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"412_gzch3m8t"},"source":["\n","Text Classification with TorchText\n","==================================\n","\n","This tutorial shows how to use the text classification datasets\n","in ``torchtext``, including\n","\n","::\n","\n","   - AG_NEWS,\n","   - SogouNews,\n","   - DBpedia,\n","   - YelpReviewPolarity,\n","   - YelpReviewFull,\n","   - YahooAnswers,\n","   - AmazonReviewPolarity,\n","   - AmazonReviewFull\n","\n","This example shows how to train a supervised learning algorithm for\n","classification using one of these ``TextClassification`` datasets.\n","\n","Load data with ngrams\n","---------------------\n","\n","A bag of ngrams feature is applied to capture some partial information\n","about the local word order. In practice, bi-gram or tri-gram are applied\n","to provide more benefits as word groups than only one word. An example:\n","\n","::\n","\n","   \"load data with ngrams\"\n","   Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n","   Tri-grams results: \"load data with\", \"data with ngrams\"\n","\n","``TextClassification`` Dataset supports the ngrams method. By setting\n","ngrams to 2, the example text in the dataset will be a list of single\n","words plus bi-grams string.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPI7JUL-39sD","executionInfo":{"status":"ok","timestamp":1612439774515,"user_tz":-540,"elapsed":4544,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}},"outputId":"217ad3dc-233b-4ef9-9ac9-4c88bba2900f"},"source":["!pip install torchtext==0.4"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting torchtext==0.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\r\u001b[K     |██████▏                         | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.41.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.23.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4) (0.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.10)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xh0Tb0ou4mBY","executionInfo":{"status":"ok","timestamp":1612439797461,"user_tz":-540,"elapsed":16760,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}},"outputId":"1c49fc37-becf-453d-cc86-49eb090b1e39"},"source":["from google.colab import drive\r\n","drive.mount('/gdrive', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8M1Wg5zn3m8u","executionInfo":{"status":"ok","timestamp":1612439839741,"user_tz":-540,"elapsed":39202,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}},"outputId":"013c11ad-aea2-48e1-df49-a9e9f64e7005"},"source":["import torch\n","import torchtext\n","from torchtext.datasets import text_classification\n","NGRAMS = 2\n","import os\n","if not os.path.isdir('/gdrive/MyDrive/ColabNotebooks/2_week4/data'):\n","\tos.mkdir('/gdrive/MyDrive/ColabNotebooks/2_week4/data')\n","train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n","    root='/gdrive/MyDrive/ColabNotebooks/2_week4/data', ngrams=NGRAMS, vocab=None)\n","BATCH_SIZE = 16\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["ag_news_csv.tar.gz: 11.8MB [00:00, 97.0MB/s]\n","120000lines [00:09, 13096.56lines/s]\n","120000lines [00:19, 6070.38lines/s]\n","7600lines [00:01, 6207.89lines/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"r1K1myH43m8u"},"source":["Define the model\n","----------------\n","\n","The model is composed of the\n","`EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__\n","layer and the linear layer (see the figure below). ``nn.EmbeddingBag``\n","computes the mean value of a “bag” of embeddings. The text entries here\n","have different lengths. ``nn.EmbeddingBag`` requires no padding here\n","since the text lengths are saved in offsets.\n","\n","Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n","the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n","performance and memory efficiency to process a sequence of tensors.\n","\n","![](../_static/img/text_sentiment_ngrams_model.png)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"NmJjAOQp3m8u","executionInfo":{"status":"ok","timestamp":1612439840357,"user_tz":-540,"elapsed":612,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","class TextSentiment(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super().__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZix6tjp3m8v"},"source":["Initiate an instance\n","--------------------\n","\n","The AG_NEWS dataset has four labels and therefore the number of classes\n","is four.\n","\n","::\n","\n","   1 : World\n","   2 : Sports\n","   3 : Business\n","   4 : Sci/Tec\n","\n","The vocab size is equal to the length of vocab (including single word\n","and ngrams). The number of classes is equal to the number of labels,\n","which is four in AG_NEWS case.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ONsdVW-V3m8v","executionInfo":{"status":"ok","timestamp":1612439841441,"user_tz":-540,"elapsed":1692,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}}},"source":["VOCAB_SIZE = len(train_dataset.get_vocab())\n","EMBED_DIM = 32\n","NUN_CLASS = len(train_dataset.get_labels())\n","model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrEC_O7s3m8v"},"source":["Functions used to generate batch\n","--------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pUJ_ROAp3m8w"},"source":["Since the text entries have different lengths, a custom function\n","generate_batch() is used to generate data batches and offsets. The\n","function is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\n","The input to ``collate_fn`` is a list of tensors with the size of\n","batch_size, and the ``collate_fn`` function packs them into a\n","mini-batch. Pay attention here and make sure that ``collate_fn`` is\n","declared as a top level def. This ensures that the function is available\n","in each worker.\n","\n","The text entries in the original data batch input are packed into a list\n","and concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\n","The offsets is a tensor of delimiters to represent the beginning index\n","of the individual sequence in the text tensor. Label is a tensor saving\n","the labels of individual text entries.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"P0E2m1eM3m8w","executionInfo":{"status":"ok","timestamp":1612439841442,"user_tz":-540,"elapsed":1688,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}}},"source":["def generate_batch(batch):\n","    label = torch.tensor([entry[0] for entry in batch])\n","    text = [entry[1] for entry in batch]\n","    offsets = [0] + [len(entry) for entry in text]\n","    # torch.Tensor.cumsum returns the cumulative sum\n","    # of elements in the dimension dim.\n","    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n","\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text = torch.cat(text)\n","    return text, offsets, label"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqyfhqTu3m8w"},"source":["Define functions to train the model and evaluate results.\n","---------------------------------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wbVwzWGr3m8w"},"source":["`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n","is recommended for PyTorch users, and it makes data loading in parallel\n","easily (a tutorial is\n","`here <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`__).\n","We use ``DataLoader`` here to load AG_NEWS datasets and send it to the\n","model for training/validation.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"aix0GQJg3m8x","executionInfo":{"status":"ok","timestamp":1612439841443,"user_tz":-540,"elapsed":1685,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}}},"source":["from torch.utils.data import DataLoader\n","\n","def train_func(sub_train_):\n","\n","    # Train the model\n","    train_loss = 0\n","    train_acc = 0\n","    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n","                      collate_fn=generate_batch)\n","    for i, (text, offsets, cls) in enumerate(data):\n","        optimizer.zero_grad()\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        output = model(text, offsets)\n","        loss = criterion(output, cls)\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += (output.argmax(1) == cls).sum().item()\n","\n","    # Adjust the learning rate\n","    scheduler.step()\n","\n","    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n","\n","def test(data_):\n","    loss = 0\n","    acc = 0\n","    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    for text, offsets, cls in data:\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        with torch.no_grad():\n","            output = model(text, offsets)\n","            loss = criterion(output, cls)\n","            loss += loss.item()\n","            acc += (output.argmax(1) == cls).sum().item()\n","\n","    return loss / len(data_), acc / len(data_)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4c4UWuCk3m8x"},"source":["Split the dataset and run the model\n","-----------------------------------\n","\n","Since the original AG_NEWS has no valid dataset, we split the training\n","dataset into train/valid sets with a split ratio of 0.95 (train) and\n","0.05 (valid). Here we use\n","`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\n","function in PyTorch core library.\n","\n","`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n","criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\n","It is useful when training a classification problem with C classes.\n","`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\n","implements stochastic gradient descent method as optimizer. The initial\n","learning rate is set to 4.0.\n","`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\n","is used here to adjust the learning rate through epochs.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ADo0bJ8W3m8y","executionInfo":{"status":"ok","timestamp":1612439985513,"user_tz":-540,"elapsed":145753,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}},"outputId":"08055ecc-b868-4cf4-c6da-ed1c75fe4ed7"},"source":["import time\n","from torch.utils.data.dataset import random_split\n","N_EPOCHS = 5\n","min_valid_loss = float('inf')\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n","\n","train_len = int(len(train_dataset) * 0.95)\n","sub_train_, sub_valid_ = \\\n","    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train_func(sub_train_)\n","    valid_loss, valid_acc = test(sub_valid_)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs / 60\n","    secs = secs % 60\n","\n","    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Epoch: 1  | time in 0 minutes, 28 seconds\n","\tLoss: 0.0262(train)\t|\tAcc: 84.7%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 88.9%(valid)\n","Epoch: 2  | time in 0 minutes, 28 seconds\n","\tLoss: 0.0119(train)\t|\tAcc: 93.6%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 90.1%(valid)\n","Epoch: 3  | time in 0 minutes, 28 seconds\n","\tLoss: 0.0068(train)\t|\tAcc: 96.5%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 89.8%(valid)\n","Epoch: 4  | time in 0 minutes, 28 seconds\n","\tLoss: 0.0038(train)\t|\tAcc: 98.1%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 91.6%(valid)\n","Epoch: 5  | time in 0 minutes, 28 seconds\n","\tLoss: 0.0022(train)\t|\tAcc: 99.0%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 91.4%(valid)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3CEMrUiz3m8y"},"source":["Running the model on GPU with the following information:\n","\n","Epoch: 1 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0263(train)     |       Acc: 84.5%(train)\n","       Loss: 0.0001(valid)     |       Acc: 89.0%(valid)\n","\n","\n","Epoch: 2 \\| time in 0 minutes, 10 seconds\n","\n","::\n","\n","       Loss: 0.0119(train)     |       Acc: 93.6%(train)\n","       Loss: 0.0000(valid)     |       Acc: 89.6%(valid)\n","\n","\n","Epoch: 3 \\| time in 0 minutes, 9 seconds\n","\n","::\n","\n","       Loss: 0.0069(train)     |       Acc: 96.4%(train)\n","       Loss: 0.0000(valid)     |       Acc: 90.5%(valid)\n","\n","\n","Epoch: 4 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0038(train)     |       Acc: 98.2%(train)\n","       Loss: 0.0000(valid)     |       Acc: 90.4%(valid)\n","\n","\n","Epoch: 5 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0022(train)     |       Acc: 99.0%(train)\n","       Loss: 0.0000(valid)     |       Acc: 91.0%(valid)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i87hY9cq3m8y"},"source":["Evaluate the model with test dataset\n","------------------------------------\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSUqOyF-3m8z","executionInfo":{"status":"ok","timestamp":1612439985515,"user_tz":-540,"elapsed":145752,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}},"outputId":"14ffab0d-2b06-4ad3-ea33-3908495ffd15"},"source":["print('Checking the results of test dataset...')\n","test_loss, test_acc = test(test_dataset)\n","print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Checking the results of test dataset...\n","\tLoss: 0.0002(test)\t|\tAcc: 90.6%(test)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_5GGN-gU3m8z"},"source":["Checking the results of test dataset…\n","\n","::\n","\n","       Loss: 0.0237(test)      |       Acc: 90.5%(test)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l81ynjzz3m8z"},"source":["Test on a random news\n","---------------------\n","\n","Use the best model so far and test a golf news. The label information is\n","available\n","`here <https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS>`__.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fU8V1aKh3m8z","executionInfo":{"status":"ok","timestamp":1612439985747,"user_tz":-540,"elapsed":145983,"user":{"displayName":"정현호","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-siUh3J8Hl5R-sjACTNq2-w_oetSH2dI7EOXCw=s64","userId":"14047294061024931201"}},"outputId":"c209a176-864e-4ebf-dca6-84a5a5d9a140"},"source":["import re\n","from torchtext.data.utils import ngrams_iterator\n","from torchtext.data.utils import get_tokenizer\n","\n","ag_news_label = {1 : \"World\",\n","                 2 : \"Sports\",\n","                 3 : \"Business\",\n","                 4 : \"Sci/Tec\"}\n","\n","def predict(text, model, vocab, ngrams):\n","    tokenizer = get_tokenizer(\"basic_english\")\n","    with torch.no_grad():\n","        text = torch.tensor([vocab[token]\n","                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n","        output = model(text, torch.tensor([0]))\n","        return output.argmax(1).item() + 1\n","\n","ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n","    enduring the season’s worst weather conditions on Sunday at The \\\n","    Open on his way to a closing 75 at Royal Portrush, which \\\n","    considering the wind and the rain was a respectable showing. \\\n","    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n","    was another story. With temperatures in the mid-80s and hardly any \\\n","    wind, the Spaniard was 13 strokes better in a flawless round. \\\n","    Thanks to his best putting performance on the PGA Tour, Rahm \\\n","    finished with an 8-under 62 for a three-stroke lead, which \\\n","    was even more impressive considering he’d never played the \\\n","    front nine at TPC Southwind.\"\n","\n","vocab = train_dataset.get_vocab()\n","model = model.to(\"cpu\")\n","\n","print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["This is a Sports news\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8SebeTMZ3m80"},"source":["This is a Sports news\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_QuGcvQA3m80"},"source":["You can find the code examples displayed in this note\n","`here <https://github.com/pytorch/text/tree/master/examples/text_classification>`__.\n","\n","\n"]}]}